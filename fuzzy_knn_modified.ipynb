{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cbb519-c7c9-4bd6-9525-4e33cab07ce1",
   "metadata": {},
   "source": [
    "# Fuzzy KNN modificado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a72fd0-13d2-4820-845f-59ef5747c61d",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a9d55d-deb4-4fa2-b22c-b427439afe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Galo\\Documents\\Matematicas\\4 Mates\\TFG\\datasets.ipynb:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ipynb.fs.full.datasets import *\n",
    "\n",
    "datasets = {\"mnist_numbers\" : mnist_numbers(), \"mnist_with_clothes\" : mnist_with_clothes(), \"chinese_mnist\" : chinese_mnist(),   \\\n",
    "            \"wine\" : wine(), \"gamma_telescope\" : gamma_telescope(),   \\\n",
    "            \"image_segmentation\": image_segmentation(), \"digits_dataset\": digits_dataset(), \"breast_cancer\": load_breast_cancer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d02a63-472d-4d20-8899-d3594cb5f02e",
   "metadata": {},
   "source": [
    "## Algoritmo Fuzzy KNN modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11874d81-eae7-46d2-ac05-18b789e89117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import json\n",
    "\n",
    "def sample_truncnorm(mu, sigma, size):\n",
    "    \"\"\"\n",
    "    Draw samples from a truncated normal distribution between 0 and 1.\n",
    "    \"\"\"\n",
    "    a, b = (0 - mu) / sigma, (1 - mu) / sigma\n",
    "    return truncnorm.rvs(a, b, loc=mu, scale=sigma, size=size)\n",
    "\n",
    "\n",
    "def fuzzy_knn_modificado(X, fuzzy_y, sample, k, size, m):\n",
    "    \"\"\"\n",
    "    Fuzzy KNN classifier:\n",
    "    - X:      array-like, shape (n_samples, n_features)\n",
    "    - fuzzy_y: DataFrame with shape (n_samples, n_classes), each cell JSON-encoded (mean, variance)\n",
    "    - sample: array-like, shape (n_features,)\n",
    "    - k:      number of neighbors\n",
    "    - size:   number of samples per neighbor per class\n",
    "    - m:      distance exponent parameter\n",
    "\n",
    "    Returns:\n",
    "    - u_true: array, fuzzy-membership probabilities for each class\n",
    "    - classes: list of class labels\n",
    "    - nn_info: array, shape (k,2) of neighbor indices and distances\n",
    "    \"\"\"\n",
    "    # 1) compute all distances\n",
    "    dists = np.linalg.norm(X - sample, axis=1)\n",
    "\n",
    "    # 2) get k nearest neighbors\n",
    "    nn_idx  = np.argsort(dists)[:k]\n",
    "    nn_dists = dists[nn_idx]\n",
    "\n",
    "    # 3) sample fuzzy-membership for each neighbor\n",
    "    dic = {}\n",
    "    for idx in nn_idx:\n",
    "        samples_per_class = []\n",
    "        for clase in fuzzy_y.columns:\n",
    "            mu, var = json.loads(fuzzy_y.loc[idx, clase])\n",
    "            if var < 1e-8:\n",
    "                v = np.full(size, mu)\n",
    "            else:\n",
    "                v = sample_truncnorm(mu, var, size)\n",
    "            samples_per_class.append(v)\n",
    "            \n",
    "        Y = np.vstack(samples_per_class)\n",
    "        # normalize columns\n",
    "        Y /= Y.sum(axis=0, keepdims=True)\n",
    "        dic[idx] = Y\n",
    "    # print(dic)\n",
    "\n",
    "    # 4) compute inverse-distance weights without division-by-zero\n",
    "    exp = 2.0 / (m - 1)\n",
    "    # print(nn_dists)\n",
    "    inv_dist = np.full_like(nn_dists, fill_value=1e4, dtype=float)\n",
    "    # print(inv_dist)\n",
    "    # only divide where distances != 0\n",
    "    np.divide(\n",
    "        1.0,\n",
    "        nn_dists**exp,\n",
    "        out=inv_dist,\n",
    "        where=(nn_dists != 0)\n",
    "    )\n",
    "    # print(inv_dist)\n",
    "\n",
    "    # 5) iterate over all combinations of draws\n",
    "    cont = np.zeros(k, dtype=int)\n",
    "    total_u = np.zeros(len(fuzzy_y.columns))\n",
    "\n",
    "    def inc_counter(counter):\n",
    "        for i in range(len(counter)):\n",
    "            if counter[i] < size - 1:\n",
    "                counter[i] += 1\n",
    "                return True\n",
    "            counter[i] = 0\n",
    "        return False\n",
    "\n",
    "    while True:\n",
    "        # accumulate weighted memberships\n",
    "        # u = np.zeros_like(total_u)\n",
    "        for i, idx in enumerate(nn_idx):\n",
    "            total_u += dic[idx][:, cont[i]] * inv_dist[i]\n",
    "        \n",
    "        # total_u += u / inv_dist.sum()\n",
    "        # total_u += u\n",
    "        if not inc_counter(cont):\n",
    "            break\n",
    "        # print(cont)\n",
    "\n",
    "    # print(cont)\n",
    "    total_u = total_u / inv_dist.sum()\n",
    "\n",
    "    # normalize by total number of combinations\n",
    "    u_true = total_u / (size ** k)\n",
    "\n",
    "    nn_info = np.vstack((nn_idx, nn_dists)).T\n",
    "    return u_true, list(fuzzy_y.columns), nn_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606208cb-26b7-4c71-98cc-759b3d073bff",
   "metadata": {},
   "source": [
    "## Intervalos de confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367efd7f-1226-4473-b258-05c99c4b3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_student(data,confidence_level):\n",
    "\n",
    "    \"\"\"\n",
    "    Funcion que dados un vector con valores, te calcula el intervalo de confianza mediante la T de student\n",
    "    data : vector con los valores\n",
    "    confidence_level : nivel de confianza del intervalo\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.stats import t\n",
    "    \n",
    "    # Calculate sample mean and standard deviation\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "    n = len(data)  # Sample size\n",
    "    \n",
    "    alpha = 1 - confidence_level\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = n - 1\n",
    "    \n",
    "    # t critical value for two-tailed test\n",
    "    t_critical = t.ppf(1 - alpha/2, df)\n",
    "    \n",
    "    # Margin of error\n",
    "    margin_of_error = t_critical * (std_dev / np.sqrt(n))\n",
    "    \n",
    "    # Confidence interval\n",
    "    confidence_interval = (mean - margin_of_error, mean + margin_of_error)\n",
    "    return confidence_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954e052-6707-4ead-84b0-1a9ecbb74e1c",
   "metadata": {},
   "source": [
    "## Experimentos para el analisis de la Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd40593-4e41-4b79-87b6-9e0e94413130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from ipynb.fs.full.datasets import * \n",
    "import logging\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "dataset_used = [\"digits_dataset\",\"breast_cancer\",\"image_segmentation\",\"gamma_telescope\",\"wine\"]\n",
    "noise_levels = [0.0,0.15,0.3,0.45]\n",
    "algorithms_used = [\"SGDClassifier\",\"gaussianNB\",\"random_forest\",\"mlp_classifier\",\"logistic_regression\"]\n",
    "\n",
    "def experimentos_no_realizados():\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que parte de una base de experimentos en un excel llamado \"experimentos.xlsx\", y va realizando los experimentos no empezados\n",
    "    de forma secuencial\n",
    "    \"\"\"\n",
    "\n",
    "    cwd = 'C:\\\\Users\\\\Galo\\\\Documents\\\\Matematicas\\\\4 Mates\\\\TFG\\\\Results fase 2\\\\'\n",
    "   \n",
    "    experimentos = pd.read_excel(\"analisis_fase_2.xlsx\")\n",
    "    experimentos_no_hechos = experimentos[experimentos[\"FECHA\"].isna()]\n",
    "    # print(experimentos_no_hechos.head())\n",
    "\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger(\"my_logger\")\n",
    "    logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "    \n",
    "    # Create a FileHandler to write logs to a file\n",
    "    file_handler = logging.FileHandler(\"analisis_fase_2.log\")\n",
    "    file_handler.setLevel(logging.DEBUG)  # Set handler logging level\n",
    "    \n",
    "    # Create a log message format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)  # Attach formatter to handler\n",
    "    \n",
    "    # Add handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    for index, row in experimentos_no_hechos.iterrows():\n",
    "\n",
    "        #Datasets elegidos para el experimento\n",
    "        if experimentos.at[index, \"DATASETS\"] == \"all\":\n",
    "            # Se genera una lista con los nombres de todos los datasets\n",
    "            datasets_exp = list(datasets.keys())\n",
    "        else:\n",
    "            # Obtenemos una lista con los nombres de los datasets seleccionados para ese experimento\n",
    "            datasets_exp = experimentos_no_hechos.at[index,\"DATASETS\"].split(\",\")\n",
    "\n",
    "        # Parametros que utilizaremos para el experimento\n",
    "        parametros = [float(num) for num in str(experimentos_no_hechos.at[index,\"PARAMETROS\"]).split(\",\")]\n",
    "        iter = [int(num) for num in str(experimentos_no_hechos.at[index,\"ITERACIONES\"]).split(\",\")]\n",
    "        # print(iter)\n",
    "        # print(parametros)\n",
    "        # print(experimentos_no_hechos.at[index,\"METODO\"])\n",
    "\n",
    "        #Metodo que utilizaremos durante el experimento\n",
    "        metodo = experimentos_no_hechos.at[index,\"METODO\"]\n",
    "\n",
    "        #Empezamos el experimento apuntando en el archivo \".log\" lo que vamos a realizar\n",
    "        exp_str = f\"Empezamos el analisis de la fase 2 con el método \" + metodo + \" y parametros \" + experimentos_no_hechos.at[index,\"PARAMETROS\"]\n",
    "        logger.critical(exp_str)\n",
    "\n",
    "        #Tiempo actual\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        #Parámetros\n",
    "        noise = parametros[0]\n",
    "        k = int(parametros[1])\n",
    "        size = int(parametros[2])\n",
    "        m = int(parametros[3])\n",
    "        \n",
    "\n",
    "        repes = iter[0]\n",
    "        repes2 = iter[1]\n",
    "        \n",
    "        fuzzy_y = pd.read_csv(cwd + \"normal_\" + datasets_exp[0] + \"_\" + str(noise) + \"_\" + metodo + \".csv\")\n",
    "        df = datasets[datasets_exp[0]]\n",
    "        X_columns = [c for c in df.columns if c!=\"label\"] \n",
    "        y = df[\"label\"].to_numpy() #Etiquetas\n",
    "        X = df[X_columns] #Atributos\n",
    "        results = {}\n",
    "        kappas_results = {\"kappas\" : []}\n",
    "        for j in range(repes):\n",
    "            logger.info(f\"Iteracion {j}\")\n",
    "            for i in range(repes2):\n",
    "                logger.debug(f\"{i}\")\n",
    "                row_to_drop = random.choice(X.index)\n",
    "                sample = X.iloc[row_to_drop,].to_numpy()\n",
    "                X_mod = X.drop(index=row_to_drop).to_numpy()\n",
    "                fuzzy_y_mod = fuzzy_y.drop(index=row_to_drop).reset_index(drop=True)\n",
    "                \n",
    "                u, clases, k_vectors = fuzzy_knn_modificado(X_mod, fuzzy_y_mod, sample, k, size, m)\n",
    "                index_max = np.argmax(u)\n",
    "\n",
    "                clase_predicha = int(clases[index_max])\n",
    "                clase_real = int(y[row_to_drop])\n",
    "\n",
    "                if i == 0 and j == 0:\n",
    "                    for t in range(len(clases)):\n",
    "                        results[clases[t]] = [u[t]] \n",
    "                    results[\"predicha\"] = [clase_predicha]\n",
    "                    results[\"real\"] = [clase_real]\n",
    "                else:\n",
    "                    for t in range(len(clases)):\n",
    "                        results[clases[t]].append(u[t])\n",
    "                    results[\"predicha\"].append(clase_predicha)\n",
    "                    results[\"real\"].append(clase_real)\n",
    "                # print(results)\n",
    "                # print([clase_predicha,clase_real])\n",
    "            kappa = cohen_kappa_score(results[\"predicha\"][-100:], results[\"real\"][-100:])\n",
    "            kappas_results[\"kappas\"].append(kappa) \n",
    "        df_results = pd.DataFrame(results)\n",
    "        filename = datasets_exp[0] + \"_\" + str(noise) + \"_\" + metodo\n",
    "        df_results.to_csv(filename + \".csv\", index=False)   \n",
    "        # Write dictionary to a JSON file\n",
    "        kappas_results[\"IC\"] = t_student(kappas_results[\"kappas\"],0.05)\n",
    "        with open(filename + \".json\", \"w\") as json_file:\n",
    "            json.dump(kappas_results, json_file, indent=4)\n",
    "            \n",
    "        # Actualizamos el dataframe con el registro de los experimentos\n",
    "        experimentos.loc[index, \"FECHA\"] = date_time  \n",
    "        experimentos.loc[index, \"NOMBRE ARCHIVO RESULTADOS\"] = filename\n",
    "\n",
    "        #Una vez finalizado el experimento, actualizamos el excel con los resultados obtenidos\n",
    "        experimentos.to_excel(\"analisis_fase_2.xlsx\", index=False)\n",
    "\n",
    "    file_handler.close()\n",
    "    logger.removeHandler(file_handler)\n",
    "    print(\"Terminados todos los experimentos\")\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a76029c3-ceaf-4bdb-bfb0-202e3806ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminados todos los experimentos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentos_no_realizados()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a419f-e132-40a0-91d2-73001b9c35b5",
   "metadata": {},
   "source": [
    "## Funcion para cerrar los handlers cuando surga un error en el código (no forma parte del programa si no hay fallos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ecde943-cbb2-41c4-829b-5cc425df6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def close_all_handlers():\n",
    "    \"\"\"Closes and removes all handlers from all loggers.\"\"\"\n",
    "    for logger_name in logging.root.manager.loggerDict:\n",
    "        logger = logging.getLogger(logger_name)\n",
    "        if hasattr(logger, \"handlers\"):  # Check if the logger has handlers\n",
    "            for handler in logger.handlers[:]:  # Iterate over a copy of the handlers list\n",
    "                handler.close()\n",
    "                logger.removeHandler(handler)\n",
    "\n",
    "# Example usage\n",
    "close_all_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3aa180-0bee-42da-b1ff-4864e2d4e429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
