{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ed68f1-a681-4d7f-b64a-db4ef4203df9",
   "metadata": {},
   "source": [
    "# Fase 2 Experimentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808ced4-32ba-403b-8fe2-f74eba319472",
   "metadata": {},
   "source": [
    "## Datasets experimentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6bedb3f-aa4c-4649-8e5c-ce0d564a9f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Galo\\Documents\\Matematicas\\4 Mates\\TFG\\datasets.ipynb:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"    return df\"\n"
     ]
    }
   ],
   "source": [
    "global datasets, metodos\n",
    "from ipynb.fs.full.datasets import *\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#Diccionario con los datasets de los experimentos\n",
    "datasets = {\"mnist_numbers\" : mnist_numbers(), \"mnist_with_clothes\" : mnist_with_clothes(), \"chinese_mnist\" : chinese_mnist(),   \\\n",
    "            \"wine\" : wine(), \"gamma_telescope\" : gamma_telescope(),   \\\n",
    "            \"image_segmentation\": image_segmentation(), \"digits_dataset\": digits_dataset(), \"breast_cancer\": load_breast_cancer()}\n",
    "\n",
    "\n",
    "# Algoritmos de clasificación utilizados durante los experimentos de la libreria sklearn\n",
    "metodos = {\"SGDClassifier\" : SGDClassifier(), \"gaussianNB\": GaussianNB(), \"random_forest\": RandomForestClassifier(),  \\\n",
    "           \"logistic_regression\": LogisticRegression(),\"xgboost\": GradientBoostingClassifier(), \"mlp_classifier\" : MLPClassifier()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5301b36-06ba-4821-bab9-4b09bfd8665b",
   "metadata": {},
   "source": [
    "## Etiquetas fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36acd30b-8bf1-45b5-badb-99705fdb0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_labels(X, y, metodo, num_veces, n, path, logger):\n",
    "    import csv\n",
    "    from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    Función que obtiene etiquetas fuzzy para un metodo y dataset\n",
    "    \"\"\"\n",
    "    clases = np.unique(y)\n",
    "    n_splits = 5\n",
    "    for iter in range(n):    \n",
    "        \n",
    "        val_predict = np.zeros((num_veces,len(y)))\n",
    "        \n",
    "        for i in range(num_veces):\n",
    "            logger.info(f\"Iteracion{iter}\")\n",
    "            logger.info(f\"Partición {i} del dataset\")\n",
    "            \n",
    "            particiones = StratifiedKFold(n_splits=n_splits, shuffle=True)     \n",
    "            \n",
    "            for j, (train_index, test_index) in enumerate(particiones.split(X, y)):\n",
    "\n",
    "                logger.debug(f\"Particion test {j}\")\n",
    "                \n",
    "                #Conjunto de datos de entrenamiento sin filtrar\n",
    "                X_train = X[train_index,]\n",
    "                y_train = y[train_index]\n",
    "    \n",
    "                #Datos del test\n",
    "                X_test = X[test_index,]\n",
    "                y_test = y[test_index]\n",
    "                \n",
    "                # generar modelo y predecir con los datos filtrados\n",
    "                modelo = metodo.fit(X_train,y_train)         \n",
    "                y_pred = modelo.predict(X_test) #  predecir solo el test\n",
    "                val_predict[i,test_index] = y_pred\n",
    "        if iter == 0:\n",
    "            data_probs = calculate_proportions(val_predict)\n",
    "        else:\n",
    "            probs = calculate_proportions(val_predict)\n",
    "            data_probs = change_probs(data_probs,probs,clases)\n",
    "    logger.info(\"Pasamos los datos a un csv\")\n",
    "    df = normal(data_probs,clases,n)\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "def change_probs(data_probs,probs,clases):\n",
    "    for i in range(len(data_probs)):\n",
    "        for clase in clases:\n",
    "            if clase in probs[i]:\n",
    "                if clase in data_probs[i]:\n",
    "                    data_probs[i][clase] =  data_probs[i][clase]+probs[i][clase]\n",
    "                else:\n",
    "                    data_probs[i][clase] =  probs[i][clase]\n",
    "    return data_probs\n",
    "        \n",
    "def calculate_proportions(matrix):\n",
    "    import numpy as np\n",
    "    proportions = []\n",
    "    for row in matrix.T:\n",
    "        unique, counts = np.unique(row, return_counts=True)\n",
    "        total_elements = row.size\n",
    "        row_proportions = {val: [count / total_elements] for val, count in zip(unique, counts)}\n",
    "        proportions.append(row_proportions)\n",
    "    return proportions   \n",
    "\n",
    "def normal(data_probs,clases,n):\n",
    "    \n",
    "    import pandas as pd \n",
    "    import numpy as np\n",
    "    \n",
    "    df = pd.DataFrame(columns=clases)\n",
    "    dist = {clase: [] for clase in clases}\n",
    "    for data in data_probs:\n",
    "        for clase in clases:\n",
    "            if clase in data: \n",
    "                padded = data[clase] + (n-len(data[clase]))*[0]\n",
    "                normal = [np.mean(padded),np.var(padded,ddof=1)]\n",
    "            else:\n",
    "                normal = [0,0]\n",
    "            dist[clase].append(normal)\n",
    "    df = pd.DataFrame(dist) \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e986b474-cdd4-4583-a3ed-d4a966982e66",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]])\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(val_predict[:,[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m      6\u001b[0m calculate_proportions(val_predict)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "X = np.array([[1,2],[3,4]])\n",
    "y = np.array([0,1])\n",
    "print(val_predict[:,[2,1]])\n",
    "calculate_proportions(val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200a98be-4889-43c8-b859-6be69eeed4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 6 8 8]\n",
      " [0 7 7 6]\n",
      " [9 6 5 6]\n",
      " [9 5 6 3]]\n",
      "[2 3]\n",
      "[7 6]\n",
      "[0 1]\n",
      "[0 7]\n"
     ]
    }
   ],
   "source": [
    "particiones = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "X = np.random.randint(0, 10, size=(4, 4))\n",
    "y = np.array([0,1,0,1])\n",
    "print(X)\n",
    "for j, (train_index, test_index) in enumerate(particiones.split(X, y)):\n",
    "    print(test_index)\n",
    "    print(X[1,test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb666bb4-3cc6-4085-ad50-5dcc60c13211",
   "metadata": {},
   "source": [
    "## Generates a dictionary with the file names and the files id of the google drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad119ef-fc68-4c83-8c3e-a3a4a88d18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drive_files_dict(folder_id):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Given a public Google Drive folder ID, fetch the embedded folder view\n",
    "    and create a dictionary mapping each file name to its file ID.\n",
    "    \"\"\"\n",
    "    # URL for the embedded folder view\n",
    "    folder_url = f\"https://drive.google.com/embeddedfolderview?id={folder_id}\"\n",
    "    response = requests.get(folder_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch the folder page. Make sure the folder is public.\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    files_dict = {}\n",
    "    \n",
    "    # Iterate through all links; each should represent a file in the folder\n",
    "    for link in soup.find_all('a'):\n",
    "        file_name = link.text.strip()\n",
    "        href = link.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "        \n",
    "        # The href should include the file id in the format: /file/d/FILE_ID/view\n",
    "        match = re.search(r'/file/d/([a-zA-Z0-9_-]+)/', href)\n",
    "        if match:\n",
    "            file_id = match.group(1)\n",
    "            files_dict[file_name] = file_id\n",
    "\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136badd-30d0-4d5f-bd0a-2ea7370121ac",
   "metadata": {},
   "source": [
    "## Código donde se realiza cada experimento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06694337-12f1-4076-a145-e056a1be7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento(metodo_exp, parametros, df_str, now, logger):\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import csv\n",
    "    from datetime import datetime\n",
    "    import time \n",
    "    import json\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que realiza un experimento estructurado de la siguiente manera para cada dataset->\n",
    "    1. Introduce ruido en las etiquetas del dataset\n",
    "    2. Aplica a todo el dataset el algoritmo de detección de ruido y obtiene una matriz de confusion\n",
    "    3. Aplica el algoritmo de detección de ruido y obtiene intervalos de confianza para los kappas\n",
    "    \n",
    "    Parametros de entrada de la función->\n",
    "    metodo_exp : metodo utilizado en el experimento\n",
    "    parametros : hiperparametros elegidos para el experimento\n",
    "    datasets_exp : lista con el nombre de los dataframes que se utilizaran en el experimento\n",
    "    now : fecha y hora del comienzo de experimento para utilizarlo de nombre del archivo donde se guardaran los resultados\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    #Parámetros\n",
    "    metodo = metodos.get(metodo_exp)\n",
    "    ruido = parametros\n",
    "\n",
    "    date_time = now.strftime(\"%m/%d/%Y\")\n",
    "    file_name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
    "    path = \"normal_\" + df_str + \"_\" + f\"{ruido}\" + \"_\" + metodo_exp + \"_\" + file_name + \".csv\"\n",
    "\n",
    "    df = datasets.get(df_str)\n",
    "    \n",
    "    y_column = \"label\"\n",
    "    X_columns = [c for c in df.columns if c!=y_column]\n",
    "    X = df[X_columns].to_numpy() #Atributos\n",
    "    \n",
    "    if ruido != 0:\n",
    "        path2 = \"ruido\" + file_name + \".csv\"\n",
    "        folder_id = \"15Rt7UEmWmCdeRH_v9JmBmfR7WPmzTyth\"\n",
    "        files_dict = get_drive_files_dict(folder_id)\n",
    "        file_name = f\"ruido_en_{df_str}_{ruido}.csv\"\n",
    "        file_id = files_dict.get(file_name)\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"  # Replace with actual file ID\n",
    "        noise_df = pd.read_csv(url)\n",
    "        y = noise_df[\"Etiquetas con ruido\"].to_numpy()\n",
    "    else:\n",
    "        y = df[y_column].to_numpy() #Etiquetas\n",
    "\n",
    "    logger.info(\"Dataset : \" + df_str)\n",
    "\n",
    "    num_veces = 10\n",
    "    n = 20\n",
    "    \n",
    "    fuzzy_labels(X, y, metodo, num_veces, n, path, logger)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Convert to hours, minutes, and seconds\n",
    "    hours = int(elapsed_time // 3600)\n",
    "    minutes = int((elapsed_time % 3600) // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "\n",
    "    tiempo = f\"{hours}h_{minutes}m_{seconds}s\"\n",
    "    \n",
    "    return path, date_time, tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add65d6d-9efe-41ee-aa5f-48071ca08a05",
   "metadata": {},
   "source": [
    "## Función para hacer los experimentos en periodos de desconexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5d30e6-825b-49e3-ae8b-e498ce1b314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from ipynb.fs.full.datasets import * \n",
    "import logging\n",
    "\n",
    "def experimentos_no_realizados():\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que parte de una base de experimentos en un excel llamado \"experimentos.xlsx\", y va realizando los experimentos no empezados\n",
    "    de forma secuencial\n",
    "    \"\"\"\n",
    "   \n",
    "    experimentos = pd.read_excel(\"experimentos_fase2.xlsx\")\n",
    "    experimentos_no_hechos = experimentos[experimentos[\"FECHA\"].isna()]\n",
    "    # print(experimentos_no_hechos.head())\n",
    "\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger(\"my_logger\")\n",
    "    logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "    \n",
    "    # Create a FileHandler to write logs to a file\n",
    "    file_handler = logging.FileHandler(\"experimentos_fase2.log\")\n",
    "    file_handler.setLevel(logging.DEBUG)  # Set handler logging level\n",
    "    \n",
    "    # Create a log message format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)  # Attach formatter to handler\n",
    "    \n",
    "    # Add handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    for index, row in experimentos_no_hechos.iterrows():\n",
    "\n",
    "        #Dataset experimento\n",
    "        datasets_exp = experimentos_no_hechos.at[index,\"DATASETS\"]\n",
    "\n",
    "        # Parametros experimento\n",
    "        num = experimentos_no_hechos.at[index,\"PARAMETROS\"]\n",
    "        parametros = float(num)\n",
    "\n",
    "        #Metodo experimento\n",
    "        metodo = experimentos_no_hechos.at[index,\"METODO\"]\n",
    "\n",
    "        #Empezamos el experimento apuntando en el archivo \".log\" lo que vamos a realizar\n",
    "        exp_str = f\"Empezamos el experimento con el método \" + metodo + \" y parametros \" + f\"{num}\"\n",
    "        logger.critical(exp_str)\n",
    "\n",
    "        #Tiempo actual\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Realizamos el experimento\n",
    "        file_name, date_time, time = experimento(metodo,parametros,datasets_exp,now,logger)\n",
    "\n",
    "        # Actualizamos el dataframe con el registro de los experimentos\n",
    "        experimentos.loc[index, \"FECHA\"] = date_time  \n",
    "        experimentos.loc[index, \"NOMBRE ARCHIVO\"] = file_name\n",
    "        experimentos.loc[index, \"TIEMPO\"] = time\n",
    "\n",
    "        #Una vez finalizado el experimento, actualizamos el excel con los resultados obtenidos\n",
    "        experimentos.to_excel(\"experimentos_fase2.xlsx\", index=False)\n",
    "    file_handler.close()\n",
    "    logger.removeHandler(file_handler)\n",
    "    print(\"Terminados todos los experimentos\")\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4bb0b70-9148-413c-95b5-d2724d210573",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m experimentos_no_realizados()\n",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m, in \u001b[0;36mexperimentos_no_realizados\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Realizamos el experimento\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m file_name, date_time, time \u001b[38;5;241m=\u001b[39m experimento(metodo,parametros,datasets_exp,now,logger)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Actualizamos el dataframe con el registro de los experimentos\u001b[39;00m\n\u001b[0;32m     55\u001b[0m experimentos\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFECHA\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m date_time  \n",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m, in \u001b[0;36mexperimento\u001b[1;34m(metodo_exp, parametros, df_str, now, logger)\u001b[0m\n\u001b[0;32m     53\u001b[0m num_veces \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     54\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m---> 56\u001b[0m fuzzy_labels(X, y, metodo, num_veces, n, path, logger)\n\u001b[0;32m     58\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Calculate elapsed time\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m, in \u001b[0;36mfuzzy_labels\u001b[1;34m(X, y, metodo, num_veces, n, path, logger)\u001b[0m\n\u001b[0;32m     32\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y[test_index]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# generar modelo y predecir con los datos filtrados\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m modelo \u001b[38;5;241m=\u001b[39m metodo\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)         \n\u001b[0;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m modelo\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;66;03m#  predecir solo el test\u001b[39;00m\n\u001b[0;32m     37\u001b[0m val_predict[i,test_index] \u001b[38;5;241m=\u001b[39m y_pred\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[0;32m    195\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[0;32m    196\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experimentos_no_realizados()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a86caf-467a-4617-901d-7883a1d26b06",
   "metadata": {},
   "source": [
    "## Funcion para cerrar los handlers cuando surga un error en el código (no forma parte del programa si no hay fallos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320eca65-9358-4e23-992b-064465411def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def close_all_handlers():\n",
    "    \"\"\"Closes and removes all handlers from all loggers.\"\"\"\n",
    "    for logger_name in logging.root.manager.loggerDict:\n",
    "        logger = logging.getLogger(logger_name)\n",
    "        if hasattr(logger, \"handlers\"):  # Check if the logger has handlers\n",
    "            for handler in logger.handlers[:]:  # Iterate over a copy of the handlers list\n",
    "                handler.close()\n",
    "                logger.removeHandler(handler)\n",
    "\n",
    "# Example usage\n",
    "close_all_handlers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
