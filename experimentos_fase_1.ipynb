{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29395946-6c2d-4282-abdf-8eac2b615acc",
   "metadata": {},
   "source": [
    "# Terminal de Experimentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a958f932-c08d-4cb0-8160-49b186ba98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experimento_con_ruido:\n",
    "    \n",
    "    #Dataframe de entrada\n",
    "    def __init__(self, df, y_column, metodo,df_name):\n",
    "        #Librerias que utilizaremos \n",
    "        import warnings\n",
    "        from sklearn.exceptions import ConvergenceWarning\n",
    "        from pandas import Series\n",
    "        import numpy as np\n",
    "        warnings.filterwarnings(\"ignore\", category = ConvergenceWarning, module = \"sklearn\")\n",
    "        \n",
    "        \"\"\"\n",
    "        X : atributos\n",
    "        y : etiquetas\n",
    "        \"\"\"\n",
    "        \n",
    "        X_columns = [c for c in df.columns if c!=y_column] \n",
    "        self.y = df[y_column].to_numpy() #Etiquetas\n",
    "        self.X = df[X_columns].to_numpy() #Atributos\n",
    "        self.metodo = metodo #Metodo de aprendizaje automático de la libreria sklearn\n",
    "        # self.path = path #Lugar donde se guardaran los documentos en el ordenador\n",
    "        self.clases = np.unique(self.y) #clases de los atributos\n",
    "        self.df_name = df_name #nombre del dataset que utilizaremos durante el experimento\n",
    "\n",
    "    def meter_ruido(self,pr):\n",
    "        \n",
    "        import numpy as np\n",
    "        import copy\n",
    "        import random\n",
    "        import os\n",
    "        \n",
    "        \"\"\"\n",
    "        Función que mete ruido en unas etiquetas dadas\n",
    "        y : etiquetas donde queremos meter el ruido\n",
    "        pr : porcentaje de ruido que metemos en las etiquetas\n",
    "        \"\"\"\n",
    "        folder_id = \"15Rt7UEmWmCdeRH_v9JmBmfR7WPmzTyth\"\n",
    "        files_dict = get_drive_files_dict(folder_id)\n",
    "        file_name = f\"ruido_en_{self.df_name}_{pr}.csv\"\n",
    "        file_id = files_dict.get(file_name)\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"  # Replace with actual file ID\n",
    "        noise_df = pd.read_csv(url)\n",
    "        self.y_mal = noise_df[\"Etiquetas con ruido\"].to_numpy()\n",
    "        self.ruido = noise_df[\"Valores donde hay ruido\"].to_numpy()\n",
    "\n",
    "\n",
    "    #Hay que arreglar lo del self.classes para que no haya redundancia entre las funciones de evaluar y meter_ruido\n",
    "    \n",
    "    def evaluar(self, y_test, y_pred, clases):\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report,cohen_kappa_score\n",
    "        \n",
    "        \"\"\"\n",
    "        Evalua un método de clasificación\n",
    "        y_test: valores esperados\n",
    "        y_pred: valores predichos por el model\n",
    "        modelo: el modelo dm ML\n",
    "        clases: lista posibles clases\n",
    "        \n",
    "        Devuelve: kappa de Cohen y matriz de confusion \n",
    "        Muestra por pantalla el class. report\n",
    "        \"\"\"\n",
    "        k =  cohen_kappa_score(y_test,y_pred)\n",
    "        print(\"kappa \",k)\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=clases)\n",
    "        print(cm)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                    display_labels=clases)\n",
    "        disp.plot()\n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "        print(classification_report(y_test, y_pred))\n",
    "        return k,cm\n",
    "\n",
    "\n",
    "    def aplica_y_evalúa_método(self,pr,test):\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "        \"\"\"\n",
    "        Devuelve las gráficas PvsP, PvsD, DvsD, DvsP\n",
    "        pr : proporción ruido en las etiquetas\n",
    "        test : proporción de datos que utilizaremos para el test\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1 preparar train y test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size= test)\n",
    "    \n",
    "        y_test_mal = self.meter_ruido(y_test, pr)\n",
    "        y_train_mal = self.meter_ruido(y_train, pr)\n",
    "    \n",
    "        \n",
    "        # 2 método y entrenamiento\n",
    "        modelo = self.metodo.fit(X_train,y_train)\n",
    "        \n",
    "        # 3 evaluar\n",
    "        y_pred = modelo.predict(X_test)  \n",
    "    \n",
    "        #PvsP\n",
    "        print(\"PvsP\")\n",
    "        self.evaluar(y_test,y_pred,modelo.classes_)\n",
    "    \n",
    "        #PvsD\n",
    "        print(\"PvsD\")\n",
    "        self.evaluar(y_test_mal,y_pred,modelo.classes_)\n",
    "    \n",
    "        # 2 método y entrenamiento\n",
    "        modelo2 = self.metodo.fit(X_train,y_train_mal)\n",
    "    \n",
    "        # 3 evaluar\n",
    "        y_pred_mal = modelo2.predict(X_test)\n",
    "        \n",
    "        #DvsD\n",
    "        print(\"DvsD\")\n",
    "        self.evaluar(y_test_mal,y_pred_mal,modelo2.classes_)\n",
    "        \n",
    "        #DvsP\n",
    "        print(\"DvsP\")\n",
    "        self.evaluar(y_test,y_pred_mal,modelo2.classes_)\n",
    "        return \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Estadísticas para ver si el algoritmo que hemos obtenido funciona \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def estadisticas_noise_algorithm(self, num_veces, con, metodo, logger):\n",
    "\n",
    "        \"\"\"\n",
    "        Función que devuelve una lista con sublistas de los kappas obtenidos para cada tipo de entrenamiento y test (PvsP, DvsP, PvsD y DvsD)\n",
    "        num_veces : numero de veces que se aplica el algoritmo \n",
    "        con : numero entre [0,1] que mide del num_veces cuantos los tienen que detectar como error para que se reporte como ruido\n",
    "        metodo : algoritmo de aprendizaje automático de clasificación utilizado\n",
    "        \"\"\"\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        import csv\n",
    "        from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from sklearn.metrics import cohen_kappa_score\n",
    "        import numpy as np\n",
    "        \n",
    "        kappa_list_pd = [] # p versus d\n",
    "        kappa_list_dd = [] # d versus d\n",
    "        kappa_list_pp = [] # p versus p\n",
    "        kappa_list_dp = [] # d versus p\n",
    "        n_splits = 5\n",
    "        \n",
    "        for i in range(num_veces):\n",
    "            particiones = StratifiedKFold(n_splits=n_splits, shuffle=True)     \n",
    "            \n",
    "            for j, (train_index, test_index) in enumerate(particiones.split(self.X, self.y)):\n",
    "                logger.info(f\"Partición {i} del dataset para obtener kappas\")\n",
    "                logger.info(f\"Particion test {j}\")\n",
    "                \n",
    "                #Conjunto de datos de entrenamiento sin filtrar\n",
    "                X_train_raw = self.X[train_index,]\n",
    "                y_train_raw = self.y_mal[train_index]\n",
    "                \n",
    "                #Aplicamos el método para detectar el ruido\n",
    "                errores = noise_detection_algorithm(X_train_raw, y_train_raw, num_veces, con, metodo, logger) \n",
    "                buenos = np.where(errores == False)[0]\n",
    "\n",
    "                #Conjunto de entrenamiento con los datos filtrados\n",
    "                X_train = X_train_raw[buenos,]\n",
    "                y_train = y_train_raw[buenos]\n",
    "\n",
    "                #Datos del test\n",
    "                X_test = self.X[test_index,]\n",
    "                y_test = self.y[test_index]\n",
    "                y_test_mal = self.y_mal[test_index] #no nos asegura que haya un porcentaje de error constante, puede variar mucho, importa en algo?\n",
    "                \n",
    "                # generar modelo y predecir con los datos filtrados\n",
    "                modelo = self.metodo.fit(X_train,y_train)         \n",
    "                y_pred = modelo.predict(X_test) #  predecir solo el test\n",
    "\n",
    "                #Cohen kappa de PvsP y PvsD\n",
    "                kappa_list_pp.append(cohen_kappa_score(y_test,y_pred))\n",
    "                kappa_list_pd.append(cohen_kappa_score(y_test_mal,y_pred))\n",
    "\n",
    "                # generar modelo y predecir con los datos sin filtrar\n",
    "                modelo2 = self.metodo.fit(X_train_raw,y_train_raw)\n",
    "                y_pred = modelo2.predict(X_test)\n",
    "\n",
    "                #Cohen kappa de DvsP y DvsD\n",
    "                kappa_list_dp.append(cohen_kappa_score(y_test,y_pred))\n",
    "                kappa_list_dd.append(cohen_kappa_score(y_test_mal,y_pred))\n",
    "\n",
    "        \n",
    "        header = [\"Cohen Kappa\"]\n",
    "        data_pd = kappa_list_pd\n",
    "        data_dd = kappa_list_dd\n",
    "        data_pp = kappa_list_pp\n",
    "        data_dp = kappa_list_dp\n",
    "          \n",
    "        # Write data to CSV\n",
    "        # with open(self.path, mode='a', newline='') as file:\n",
    "        #     writer = csv.writer(file)\n",
    "            \n",
    "        #     # Write in the csv\n",
    "        #     writer.writerow(header)\n",
    "\n",
    "        #     writer.writerow([\"Cohen kappa PvsP\"])\n",
    "        #     writer.writerows([data_pp])\n",
    "\n",
    "        #     writer.writerow([\"Cohen kappa PvsD\"])\n",
    "        #     writer.writerows([data_pd])\n",
    "\n",
    "        #     writer.writerow([\"Cohen kappa DvsP\"])\n",
    "        #     writer.writerows([data_dp])\n",
    "            \n",
    "        #     writer.writerow([\"Cohen kappa DvsD\"])\n",
    "        #     writer.writerows([data_dd])                        \n",
    "\n",
    "        return [data_pp,data_pd,data_dp,data_dd]\n",
    "\n",
    "    def est(self,logger):\n",
    "\n",
    "        \"\"\"\n",
    "        Funcion que calcula los kappa para el dataset completo sin y con ruido\n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import csv\n",
    "        kappa1 = []\n",
    "        kappa2 = []\n",
    "        \n",
    "        #Imprime por pantalla los resultados obtenidos con la matriz de confusion\n",
    "        for i in range(10):\n",
    "            logger.info(f\"Calculando kappa dataset completo sin ruido, {i}\")\n",
    "            kappa1.append(kappa_dataset_completo(self.X, self.y, 10, self.metodo))\n",
    "            logger.info(f\"Calculando kappa dataset completo con ruido, {i}\")\n",
    "            kappa2.append(kappa_dataset_completo(self.X, self.y_mal, 10, self.metodo))\n",
    "        \n",
    "        # Asocia la matriz de confusion a la variable data\n",
    "        # header1 = [\"Kappa dataset completo sin ruido\"]\n",
    "        # header2 = [\"Kappa dataset completo con ruido\"] \n",
    "        \n",
    "        # # Comentario : Falta poner para saber cual es cual en confusion matrix\n",
    "        # # Añdimos la excel la matriz de confusión\n",
    "        # with open(self.path, mode='a', newline='') as file:\n",
    "        #     writer = csv.writer(file)\n",
    "            \n",
    "        #     # Write header (optional)\n",
    "        #     writer.writerow(header1)\n",
    "            \n",
    "        #     # Write the data\n",
    "        #     writer.writerow([kappa1])\n",
    "\n",
    "        #     # Write header (optional)\n",
    "        #     writer.writerow(header2)\n",
    "            \n",
    "        #     # Write the data\n",
    "        #     writer.writerow([kappa2])\n",
    "\n",
    "        return kappa1,kappa2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8f3ea-83a9-4ddb-9abe-4a497e66f203",
   "metadata": {},
   "source": [
    "## Generates a dictionary with the file names and the files id of the google drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8986630d-1234-4b0b-932f-2b7f0d411ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drive_files_dict(folder_id):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Given a public Google Drive folder ID, fetch the embedded folder view\n",
    "    and create a dictionary mapping each file name to its file ID.\n",
    "    \"\"\"\n",
    "    # URL for the embedded folder view\n",
    "    folder_url = f\"https://drive.google.com/embeddedfolderview?id={folder_id}\"\n",
    "    response = requests.get(folder_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch the folder page. Make sure the folder is public.\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    files_dict = {}\n",
    "    \n",
    "    # Iterate through all links; each should represent a file in the folder\n",
    "    for link in soup.find_all('a'):\n",
    "        file_name = link.text.strip()\n",
    "        href = link.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "        \n",
    "        # The href should include the file id in the format: /file/d/FILE_ID/view\n",
    "        match = re.search(r'/file/d/([a-zA-Z0-9_-]+)/', href)\n",
    "        if match:\n",
    "            file_id = match.group(1)\n",
    "            files_dict[file_name] = file_id\n",
    "\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9aa81-459b-491d-856b-bfd91d7c94d8",
   "metadata": {},
   "source": [
    "## Algoritmo de detección de ruido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f5a900-e566-4152-811e-45b2d6282263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_detection_algorithm(X, y, num_veces, con, metodo, logger): \n",
    "        \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    Método para clasificar errores basado en el K-means:\n",
    "    num_veces : numero de veces que se aplica el algoritmo\n",
    "    pr : 0 en caso de que el dataframe no tenga ruida/ <0 para meter ruido en el dataframe con esa proporción\n",
    "    con : numero entre [0,1] que mide del num_veces cuantos los tienen que detectar como error para que se reporte como ruido\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    n_mal = np.array(n*[0])\n",
    "    n_splits = 5 #utilizamos para entrenar un 80% de los datos diviendolo en 7 conjuntos o grupos de datos\n",
    "\n",
    "    for i in range(num_veces):\n",
    "        logger.debug(f\"Iteración {i}\")\n",
    "        particiones = StratifiedKFold(n_splits=n_splits, shuffle=True)     \n",
    "        for j, (train_index, test_index) in enumerate(particiones.split(X, y)):        \n",
    "            X_train = X[train_index,]\n",
    "            y_train = y[train_index]\n",
    "            X_test= X[test_index,]\n",
    "            y_test = y[test_index]\n",
    "            \n",
    "            # generar modelo y predecir\n",
    "            modelo = metodo.fit(X_train,y_train)\n",
    "            y_pred = modelo.predict(X_test)\n",
    "        \n",
    "            # apuntamos cuáles han sido mal clasificados\n",
    "            filtro = y_pred != y_test\n",
    "            i_mal = test_index[filtro]\n",
    "            n_mal[i_mal] = n_mal[i_mal]+1\n",
    "            \n",
    "    errores = n_mal > num_veces*con\n",
    "    return errores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c370d9-5fce-430e-adfe-28a44df08f38",
   "metadata": {},
   "source": [
    "## T student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548e7c47-397b-47b2-9a0b-1fdefcbe35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_student(data,confidence_level):\n",
    "\n",
    "    \"\"\"\n",
    "    Funcion que dados un vector con valores, te calcula el intervalo de confianza mediante la T de student\n",
    "    data : vector con los valores\n",
    "    confidence_level : nivel de confianza del intervalo\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.stats import t\n",
    "    \n",
    "    # Calculate sample mean and standard deviation\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "    n = len(data)  # Sample size\n",
    "    \n",
    "    alpha = 1 - confidence_level\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = n - 1\n",
    "    \n",
    "    # t critical value for two-tailed test\n",
    "    t_critical = t.ppf(1 - alpha/2, df)\n",
    "    \n",
    "    # Margin of error\n",
    "    margin_of_error = t_critical * (std_dev / np.sqrt(n))\n",
    "    \n",
    "    # Confidence interval\n",
    "    confidence_interval = (mean - margin_of_error, mean + margin_of_error)\n",
    "    return confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0675feb6-94ae-4991-9c5c-646dcad961df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8499588253632513, 0.8967440451493263)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_student([\n",
    "         0.8506387160170324,\n",
    "         0.9623140495867769,\n",
    "         0.8260427263479145,\n",
    "         0.8687068114511353,\n",
    "         0.8319286068418443,\n",
    "         0.7819572840293274,\n",
    "         0.8492561983471074,\n",
    "         0.8660624370594159,\n",
    "         0.9437314906219151,\n",
    "         0.815900944933203,\n",
    "         0.8199052132701422,\n",
    "         0.9626596790042581,\n",
    "         0.7692307692307692,\n",
    "         0.9431704885343968,\n",
    "         0.9428812131423757,\n",
    "         0.8869421487603306,\n",
    "         0.8506387160170324,\n",
    "         0.8673978065802592,\n",
    "         0.8673978065802592,\n",
    "         0.886317907444668,\n",
    "         0.8869421487603306,\n",
    "         0.9070753179002282,\n",
    "         0.797676669893514,\n",
    "         0.8869047619047619,\n",
    "         0.9621059691482227\n",
    "      ],0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e394121-a630-4aee-aa42-e4962aad208a",
   "metadata": {},
   "source": [
    "## Calcula el kappa sobre el dataframe entero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1086614b-551d-4bef-b739-0dc263b4796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_dataset_completo(X, y, k, model):\n",
    "\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from matplotlib import pyplot as plt\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    # Inicializar matriz de confusión acumulada y lista de kappa\n",
    "    # conf_matrix_total = np.zeros((len(np.unique(y)), len(np.unique(y))))\n",
    "    kappa_scores = []\n",
    "\n",
    "    # Loop sobre los folds\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index,], X[test_index,]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Entrenar modelo\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predicción\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calcular matriz de confusión y kappa\n",
    "        # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        # conf_matrix_total += conf_matrix\n",
    "        kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "    # Kappa promedio\n",
    "    kappa_mean = np.mean(kappa_scores)\n",
    "\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_total/k ,\n",
    "    #                               display_labels=model.classes_)\n",
    "    # disp.plot()\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    return kappa_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c6600-ef17-483a-85a1-aaf59ae945be",
   "metadata": {},
   "source": [
    "## Datasets y algoritmos de aprendizaje automático\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c6960-ae67-43e8-8d18-a6afbab0e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global datasets, metodos\n",
    "from ipynb.fs.full.datasets import *\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#Diccionario con los datasets de los experimentos\n",
    "datasets = {\"mnist_numbers\" : mnist_numbers(), \"mnist_with_clothes\" : mnist_with_clothes(), \"chinese_mnist\" : chinese_mnist(),   \\\n",
    "            \"wine\" : wine(), \"gamma_telescope\" : gamma_telescope(),   \\\n",
    "            \"image_segmentation\": image_segmentation(), \"digits_dataset\": digits_dataset(), \"breast_cancer\": load_breast_cancer()}\n",
    "\n",
    "# Algoritmos de clasificación utilizados durante los experimentos de la libreria sklearn\n",
    "metodos = {\"SGDClassifier\" : SGDClassifier(), \"gaussianNB\": GaussianNB(), \"random_forest\": RandomForestClassifier(),  \\\n",
    "           \"logistic_regression\": LogisticRegression(),\"xgboost\": GradientBoostingClassifier(), \"mlp_classifier\" : MLPClassifier()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4d6d6-fc21-4a43-97ec-49cf4147137e",
   "metadata": {},
   "source": [
    "# Código donde se realiza cada experimento\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9ec5a-f57f-4997-ba0d-9f77fa60d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento(metodo_exp, parametros, df_str, now, logger):\n",
    "\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import csv\n",
    "    from datetime import datetime\n",
    "    import time \n",
    "    import json\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que realiza un experimento estructurado de la siguiente manera para cada dataset->\n",
    "    1. Introduce ruido en las etiquetas del dataset\n",
    "    2. Aplica a todo el dataset el algoritmo de detección de ruido y obtiene una matriz de confusion\n",
    "    3. Aplica el algoritmo de detección de ruido y obtiene intervalos de confianza para los kappas\n",
    "    \n",
    "    Parametros de entrada de la función->\n",
    "    metodo_exp : metodo utilizado en el experimento\n",
    "    parametros : hiperparametros elegidos para el experimento\n",
    "    datasets_exp : lista con el nombre de los dataframes que se utilizaran en el experimento\n",
    "    now : fecha y hora del comienzo de experimento para utilizarlo de nombre del archivo donde se guardaran los resultados\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    #Parámetros\n",
    "    metodo = metodos.get(metodo_exp)\n",
    "    ruido = parametros[0]\n",
    "    n_val_cruz = int(parametros[1])\n",
    "    cond = parametros[2]\n",
    "    confidence_level = parametros[3]\n",
    "    list_kappa_pp = []\n",
    "    list_kappa_pd = []\n",
    "    list_kappa_dp = []\n",
    "    list_kappa_dd = []\n",
    "\n",
    "    date_time = now.strftime(\"%m/%d/%Y\")\n",
    "    file_name = now.strftime(\"_%m_%d_%Y_%H_%M_%S\")\n",
    "    path = df_str + \"_\" + f\"{ruido}\" + \"_\" + metodo_exp + \"_\" + file_name + \".json\"\n",
    "    path2 = \"ruido\" + file_name + \".csv\"\n",
    "    # open(path, \"w\") #Esto se puede poner mas adelante\n",
    "    dic = {}\n",
    "\n",
    "    logger.info(\"Dataset : \" + df_str)\n",
    "\n",
    "    df = datasets.get(df_str)\n",
    "\n",
    "    dic[\"metodo\"] = metodo_exp\n",
    "    dic[\"dataset\"] = df_str\n",
    "    dic[\"parametros\"] = parametros\n",
    "    \n",
    "    #Metemos ruido en el dataset y aplicamos el algoritmo de detección de ruido\n",
    "    \n",
    "    # mnist = experimento_con_ruido(df,\"label\",metodo,path,df_str)\n",
    "    mnist = experimento_con_ruido(df,\"label\",metodo,df_str)\n",
    "\n",
    "    logger.info(f\"Leemos el fichero con ruido de la carpeta de google drive para una proporcion de {ruido} de ruido y el dataset: \" + df_str)\n",
    "    mnist.meter_ruido(ruido)\n",
    "    \n",
    "    #Aplicamos el algoritmo a todos los datos y obtenemos la matriz de confusión\n",
    "    logger.info(\"Kappas para dataset completo con y sin ruido\")\n",
    "    # mnist.errores = noise_detection_algorithm(mnist.X, mnist.y, n_val_cruz, cond, metodo,logger)\n",
    "    kappa_sin, kappa_con = mnist.est(logger)\n",
    "\n",
    "    confidence_interval_sin = t_student(kappa_sin,confidence_level)\n",
    "    confidence_interval_con = t_student(kappa_con,confidence_level)\n",
    "    \n",
    "    #Calculamos los kappas de PvsP, DvsP, PvsD y DvsD para cada dataset y los metemos en una lista\n",
    "    logger.info(\"Estadísticas del algoritmo de detección de ruido\")\n",
    "    kappa_pp,kappa_pd,kappa_dp,kappa_dd = mnist.estadisticas_noise_algorithm(n_val_cruz, cond, metodo,logger)\n",
    "    # list_kappa_pp = list_kappa_pp + kappa_pp\n",
    "    # list_kappa_pd = list_kappa_pd + kappa_pd\n",
    "    # list_kappa_dp = list_kappa_dp + kappa_dp\n",
    "    # list_kappa_dd = list_kappa_dd + kappa_dd\n",
    "\n",
    "    #Calculamos los intervalos de confianza para cada lista de kappas\n",
    "    #PvsP\n",
    "    confidence_interval_pp = t_student(kappa_pp,confidence_level)\n",
    "    \n",
    "    #PvsD\n",
    "    confidence_interval_pd = t_student(kappa_pd,confidence_level)\n",
    "    \n",
    "    #DvsP\n",
    "    confidence_interval_dp = t_student(kappa_dp,confidence_level)\n",
    "    \n",
    "    #DvsD\n",
    "    confidence_interval_dd = t_student(kappa_dd,confidence_level)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Convert to hours, minutes, and seconds\n",
    "    hours = int(elapsed_time // 3600)\n",
    "    minutes = int((elapsed_time % 3600) // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "\n",
    "    tiempo = f\"{hours}h_{minutes}m_{seconds}s\"\n",
    "\n",
    "    dic[\"tiempo\"] = tiempo\n",
    "\n",
    "    dic[\"sin_ruido\"] = {\"kappas\":kappa_sin,\"intervalo\": confidence_interval_sin}\n",
    "    dic[\"con_ruido\"] = {\"kappas\":kappa_con,\"intervalo\": confidence_interval_con}\n",
    "\n",
    "    print(\"Kappa de Cohen promedio sin ruido:\", confidence_interval_sin)\n",
    "    print(\"Kappa de Cohen promedio con ruido:\", confidence_interval_con)\n",
    "\n",
    "    dic[\"PvsP\"] =  {\"kappas\": kappa_pp,\"intervalo\" :confidence_interval_pp}\n",
    "    dic[\"PvsD\"] =  {\"kappas\": kappa_pd,\"intervalo\" :confidence_interval_pd}\n",
    "    dic[\"DvsP\"] =  {\"kappas\": kappa_dp,\"intervalo\" :confidence_interval_dp}\n",
    "    dic[\"DvsD\"] =  {\"kappas\": kappa_dd,\"intervalo\" :confidence_interval_dd}\n",
    "\n",
    "    json_object = json.dumps(dic, indent=3)    \n",
    " \n",
    "    # Writing to sample.json\n",
    "    with open(path, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    \n",
    "    return path, date_time, path2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223ebf9-4abd-4a62-985c-b780bf785396",
   "metadata": {},
   "source": [
    "## Función para hacer los experimentos en periodos de desconexión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72355fea-6a0a-46d6-a1b3-0e6209e7e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from ipynb.fs.full.datasets import * \n",
    "import logging\n",
    "\n",
    "def experimentos_no_realizados():\n",
    "    \n",
    "    \"\"\"\n",
    "    Función que parte de una base de experimentos en un excel llamado \"experimentos.xlsx\", y va realizando los experimentos no empezados\n",
    "    de forma secuencial\n",
    "    \"\"\"\n",
    "   \n",
    "    experimentos = pd.read_excel(\"experimentos.xlsx\")\n",
    "    experimentos_no_hechos = experimentos[experimentos[\"FECHA\"].isna()]\n",
    "    # print(experimentos_no_hechos.head())\n",
    "\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger(\"my_logger\")\n",
    "    logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "    \n",
    "    # Create a FileHandler to write logs to a file\n",
    "    file_handler = logging.FileHandler(\"experimentos.log\")\n",
    "    file_handler.setLevel(logging.DEBUG)  # Set handler logging level\n",
    "    \n",
    "    # Create a log message format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)  # Attach formatter to handler\n",
    "    \n",
    "    # Add handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    for index, row in experimentos_no_hechos.iterrows():\n",
    "\n",
    "        #Datasets elegidos para el experimento\n",
    "        if experimentos.at[index, \"DATASETS\"] == \"all\":\n",
    "            # Se genera una lista con los nombres de todos los datasets\n",
    "            datasets_exp = list(datasets.keys())\n",
    "        else:\n",
    "            # Obtenemos una lista con los nombres de los datasets seleccionados para ese experimento\n",
    "            datasets_exp = experimentos_no_hechos.at[index,\"DATASETS\"].split(\",\")\n",
    "\n",
    "        # Parametros que utilizaremos para el experimento\n",
    "        parametros = [float(num) for num in str(experimentos_no_hechos.at[index,\"PARAMETROS\"]).split(\",\")]\n",
    "        # print(experimentos_no_hechos.at[index,\"METODO\"])\n",
    "\n",
    "        #Metodo que utilizaremos durante el experimento\n",
    "        metodo = experimentos_no_hechos.at[index,\"METODO\"]\n",
    "\n",
    "        #Empezamos el experimento apuntando en el archivo \".log\" lo que vamos a realizar\n",
    "        exp_str = f\"Empezamos el experimento con el método \" + metodo + \" y parametros \" + experimentos_no_hechos.at[index,\"PARAMETROS\"]\n",
    "        logger.critical(exp_str)\n",
    "\n",
    "        #Tiempo actual\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Realizamos el experimento\n",
    "        file_name, date_time, file_con_ruido = experimento(metodo,parametros,datasets_exp[0],now,logger)\n",
    "\n",
    "        # Actualizamos el dataframe con el registro de los experimentos\n",
    "        experimentos.loc[index, \"FECHA\"] = date_time  \n",
    "        experimentos.loc[index, \"NOMBRE ARCHIVO\"] = file_name\n",
    "        experimentos.loc[index, \"NOMBRE ARCHIVO RUIDO\"] = file_con_ruido\n",
    "        experimentos.loc[index, \"ITERACIONES\"] =  int(parametros[1])*5\n",
    "\n",
    "        #Una vez finalizado el experimento, actualizamos el excel con los resultados obtenidos\n",
    "        experimentos.to_excel(\"experimentos.xlsx\", index=False)\n",
    "    file_handler.close()\n",
    "    logger.removeHandler(file_handler)\n",
    "    print(\"Terminados todos los experimentos\")\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de571a15-21ca-45d4-9b2b-aea56953ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa de Cohen promedio sin ruido: (0.9250831916612469, 0.9432725736509661)\n",
      "Kappa de Cohen promedio con ruido: (0.4707757068726755, 0.5397219424637951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Galo\\AppData\\Local\\Temp\\ipykernel_33136\\3671143120.py:60: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '03/10/2025' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  experimentos.loc[index, \"FECHA\"] = date_time\n",
      "C:\\Users\\Galo\\AppData\\Local\\Temp\\ipykernel_33136\\3671143120.py:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'breast_cancer_0.15_SGDClassifier__03_10_2025_16_37_57.json' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  experimentos.loc[index, \"NOMBRE ARCHIVO\"] = file_name\n",
      "C:\\Users\\Galo\\AppData\\Local\\Temp\\ipykernel_33136\\3671143120.py:62: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'ruido_03_10_2025_16_37_57.csv' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  experimentos.loc[index, \"NOMBRE ARCHIVO RUIDO\"] = file_con_ruido\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa de Cohen promedio sin ruido: (0.9136288964829815, 0.9198965702380564)\n",
      "Kappa de Cohen promedio con ruido: (0.5997890545410725, 0.6280273673080206)\n",
      "Kappa de Cohen promedio sin ruido: (0.9272434414757298, 0.934548698637675)\n",
      "Kappa de Cohen promedio con ruido: (0.6268988772870326, 0.6398183467988653)\n",
      "Kappa de Cohen promedio sin ruido: (0.8532668485898559, 0.8602153737229691)\n",
      "Kappa de Cohen promedio con ruido: (0.5741317069061382, 0.580506160355833)\n",
      "Kappa de Cohen promedio sin ruido: (0.9344047151758943, 0.9437474683399139)\n",
      "Kappa de Cohen promedio con ruido: (0.6385434249228368, 0.6471904907305945)\n",
      "Terminados todos los experimentos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentos_no_realizados()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a8d5b-526b-4999-9a97-8fdab2225413",
   "metadata": {},
   "source": [
    "## Funcion para cerrar los handlers cuando surga un error en el código (no forma parte del programa si no hay fallos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79128ea3-4d83-4b2c-a228-3174f8230b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# def close_all_handlers():\n",
    "#     \"\"\"Closes and removes all handlers from all loggers.\"\"\"\n",
    "#     for logger_name in logging.root.manager.loggerDict:\n",
    "#         logger = logging.getLogger(logger_name)\n",
    "#         if hasattr(logger, \"handlers\"):  # Check if the logger has handlers\n",
    "#             for handler in logger.handlers[:]:  # Iterate over a copy of the handlers list\n",
    "#                 handler.close()\n",
    "#                 logger.removeHandler(handler)\n",
    "\n",
    "# # Example usage\n",
    "# close_all_handlers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c779929-27af-4a99-bf89-8fdc26430ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # #File donde se almacenara los resultados\n",
    "    # with open(path, mode='a', newline='') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow([df_str])\n",
    "\n",
    "    # with open(path, mode='a', newline='') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow([\"Confidence interval of PvsP\"])\n",
    "    #     writer.writerow(confidence_interval_pp)\n",
    "    #     writer.writerow([\"Confidence interval of PvsD\"])\n",
    "    #     writer.writerow(confidence_interval_pd)\n",
    "    #     writer.writerow([\"Confidence interval of DvsP\"])\n",
    "    #     writer.writerow(confidence_interval_dp)\n",
    "    #     writer.writerow([\"Confidence interval of DvsD\"])\n",
    "    #     writer.writerow(confidence_interval_dd)\n",
    "    \n",
    "    # #Calculamos los intervalos de confianza para cada lista de kappas\n",
    "    # #PvsP\n",
    "    # confidence_interval_pp = t_student(list_kappa_pp,confidence_level)\n",
    "    \n",
    "    # #PvsD\n",
    "    # confidence_interval_pd = t_student(list_kappa_pd,confidence_level)\n",
    "    \n",
    "    # #DvsP\n",
    "    # confidence_interval_dp = t_student(list_kappa_dp,confidence_level)\n",
    "    \n",
    "    # #DvsD\n",
    "    # confidence_interval_dd = t_student(list_kappa_dd,confidence_level)\n",
    "    \n",
    "    #Escribimos los resultados en el excel\n",
    "    # with open(path, mode='a', newline='') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow([\"Intervalos de confianza de todos los kappas y datasets\"])\n",
    "    #     writer.writerow([\"Confidence interval of PvsP\"])\n",
    "    #     writer.writerow(confidence_interval_pp)\n",
    "    #     writer.writerow([\"Confidence interval of PvsD\"])\n",
    "    #     writer.writerow(confidence_interval_pd)\n",
    "    #     writer.writerow([\"Confidence interval of DvsP\"])\n",
    "    #     writer.writerow(confidence_interval_dp)\n",
    "    #     writer.writerow([\"Confidence interval of DvsD\"])\n",
    "    #     writer.writerow(confidence_interval_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d76cbdf-61fb-413d-812b-b444a2eeb076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
